{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "N, D_in, H1,H2,H3, D_out = 10 ,3 ,4,3,4 ,2\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = torch.nn.Linear(D_in, H1)\n",
    "layer2 = torch.nn.Linear(H1, H2)\n",
    "layer3 = torch.nn.Linear(H2, H3, bias = False)\n",
    "layer4 = torch.nn.Linear(H3, D_out)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "optimizer1 = torch.optim.SGD(layer1.parameters(), lr = 0.05, momentum = 0.9)\n",
    "optimizer2 = torch.optim.SGD(layer2.parameters(), lr = 0.05, momentum = 0.9)\n",
    "optimizer3 = torch.optim.SGD(layer3.parameters(), lr = 0.05, momentum = 0.9)\n",
    "optimizer4 = torch.optim.SGD(layer4.parameters(), lr = 0.05, momentum = 0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.3311,  0.2519,  0.3068],\n",
      "        [-0.3082,  0.3149, -0.4042],\n",
      "        [-0.5510,  0.2228,  0.4530],\n",
      "        [ 0.4059,  0.2195, -0.3058]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.3336, -0.4914, -0.3525, -0.1809],\n",
      "        [ 0.2746,  0.0961,  0.2403, -0.2765],\n",
      "        [-0.1256, -0.2266, -0.2378,  0.1720]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.4106,  0.5603, -0.5586],\n",
      "        [ 0.0385, -0.1680, -0.0208],\n",
      "        [-0.0714, -0.1042,  0.0299],\n",
      "        [ 0.4249, -0.3446,  0.4906]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0623,  0.3307,  0.4338, -0.1502],\n",
      "        [ 0.0686, -0.3590, -0.4102, -0.1842]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(layer1.weight)\n",
    "print(layer2.weight)\n",
    "print(layer3.weight)\n",
    "print(layer4.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0\r",
      "before train , layer1.weight: Parameter containing:\n",
      "tensor([[-0.3311,  0.2519,  0.3068],\n",
      "        [-0.3082,  0.3149, -0.4042],\n",
      "        [-0.5510,  0.2228,  0.4530],\n",
      "        [ 0.4059,  0.2195, -0.3058]], requires_grad=True)\n",
      "loss: tensor(1.2523, grad_fn=<MseLossBackward>)\n",
      "before backward, layer1.weight.grad: None\n",
      "after backward, layer1.weight.grad: tensor([[-0.0428, -0.0323, -0.0040],\n",
      "        [-0.0381, -0.0295,  0.0014],\n",
      "        [-0.0487, -0.0371, -0.0027],\n",
      "        [ 0.0340,  0.0251,  0.0069]])\n",
      "after train, layer.weight: Parameter containing:\n",
      "tensor([[-0.3290,  0.2535,  0.3070],\n",
      "        [-0.3063,  0.3163, -0.4042],\n",
      "        [-0.5485,  0.2247,  0.4532],\n",
      "        [ 0.4042,  0.2182, -0.3061]], requires_grad=True)\n",
      "0 --------------------------------------------------\n",
      "step: 1\r",
      "before train , layer1.weight: Parameter containing:\n",
      "tensor([[-0.3290,  0.2535,  0.3070],\n",
      "        [-0.3063,  0.3163, -0.4042],\n",
      "        [-0.5485,  0.2247,  0.4532],\n",
      "        [ 0.4042,  0.2182, -0.3061]], requires_grad=True)\n",
      "loss: tensor(1.2156, grad_fn=<MseLossBackward>)\n",
      "before backward, layer1.weight.grad: tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "after backward, layer1.weight.grad: tensor([[-0.0360, -0.0289, -0.0035],\n",
      "        [-0.0319, -0.0262,  0.0022],\n",
      "        [-0.0399, -0.0323, -0.0018],\n",
      "        [ 0.0268,  0.0212,  0.0065]])\n",
      "after train, layer.weight: Parameter containing:\n",
      "tensor([[-0.3253,  0.2564,  0.3073],\n",
      "        [-0.3030,  0.3190, -0.4044],\n",
      "        [-0.5444,  0.2280,  0.4534],\n",
      "        [ 0.4013,  0.2160, -0.3068]], requires_grad=True)\n",
      "1 --------------------------------------------------\n",
      "step: 2\r",
      "before train , layer1.weight: Parameter containing:\n",
      "tensor([[-0.3253,  0.2564,  0.3073],\n",
      "        [-0.3030,  0.3190, -0.4044],\n",
      "        [-0.5444,  0.2280,  0.4534],\n",
      "        [ 0.4013,  0.2160, -0.3068]], requires_grad=True)\n",
      "loss: tensor(1.1633, grad_fn=<MseLossBackward>)\n",
      "before backward, layer1.weight.grad: tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "after backward, layer1.weight.grad: tensor([[-0.0258, -0.0231, -0.0020],\n",
      "        [-0.0232, -0.0206,  0.0042],\n",
      "        [-0.0274, -0.0245,  0.0003],\n",
      "        [ 0.0166,  0.0149,  0.0054]])\n",
      "after train, layer.weight: Parameter containing:\n",
      "tensor([[-0.3206,  0.2601,  0.3077],\n",
      "        [-0.2989,  0.3224, -0.4048],\n",
      "        [-0.5392,  0.2321,  0.4535],\n",
      "        [ 0.3979,  0.2133, -0.3076]], requires_grad=True)\n",
      "2 --------------------------------------------------\n",
      "step: 3\r",
      "before train , layer1.weight: Parameter containing:\n",
      "tensor([[-0.3206,  0.2601,  0.3077],\n",
      "        [-0.2989,  0.3224, -0.4048],\n",
      "        [-0.5392,  0.2321,  0.4535],\n",
      "        [ 0.3979,  0.2133, -0.3076]], requires_grad=True)\n",
      "loss: tensor(1.1170, grad_fn=<MseLossBackward>)\n",
      "before backward, layer1.weight.grad: tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "after backward, layer1.weight.grad: tensor([[-0.0161, -0.0160,  0.0008],\n",
      "        [-0.0154, -0.0141,  0.0075],\n",
      "        [-0.0164, -0.0158,  0.0038],\n",
      "        [ 0.0074,  0.0080,  0.0034]])\n",
      "after train, layer.weight: Parameter containing:\n",
      "tensor([[-0.3156,  0.2643,  0.3081],\n",
      "        [-0.2944,  0.3261, -0.4055],\n",
      "        [-0.5338,  0.2367,  0.4535],\n",
      "        [ 0.3944,  0.2105, -0.3085]], requires_grad=True)\n",
      "3 --------------------------------------------------\n",
      "step: 4\r",
      "before train , layer1.weight: Parameter containing:\n",
      "tensor([[-0.3156,  0.2643,  0.3081],\n",
      "        [-0.2944,  0.3261, -0.4055],\n",
      "        [-0.5338,  0.2367,  0.4535],\n",
      "        [ 0.3944,  0.2105, -0.3085]], requires_grad=True)\n",
      "loss: tensor(1.0881, grad_fn=<MseLossBackward>)\n",
      "before backward, layer1.weight.grad: tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "after backward, layer1.weight.grad: tensor([[-0.0092, -0.0087,  0.0048],\n",
      "        [-0.0104, -0.0076,  0.0119],\n",
      "        [-0.0093, -0.0076,  0.0083],\n",
      "        [ 0.0011,  0.0016,  0.0009]])\n",
      "after train, layer.weight: Parameter containing:\n",
      "tensor([[-0.3107,  0.2685,  0.3081],\n",
      "        [-0.2898,  0.3299, -0.4067],\n",
      "        [-0.5284,  0.2412,  0.4531],\n",
      "        [ 0.3913,  0.2078, -0.3094]], requires_grad=True)\n",
      "4 --------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print('step: {}'.format(i), end = '\\r')\n",
    "    print('before train , layer1.weight:',layer1.weight)\n",
    "    \n",
    "    #forward\n",
    "    hidden1 = layer1.forward(x)\n",
    "    \n",
    "    hidden2 = layer2.forward(hidden1)\n",
    "    hidden3 = layer3.forward(hidden2)\n",
    "    y_pred = layer4.forward(hidden3)\n",
    "    \n",
    "    #computing the error\n",
    "    loss = loss_fn(y_pred , y)\n",
    "    print('loss:' , loss)\n",
    "    \n",
    "    # clear gradient of last step\n",
    "    optimizer1.zero_grad()\n",
    "    optimizer2.zero_grad()\n",
    "    optimizer3.zero_grad()\n",
    "    optimizer4.zero_grad()\n",
    "    print(\"before backward, layer1.weight.grad:\" , layer1.weight.grad)\n",
    "    \n",
    "    # backward\n",
    "    loss.backward()\n",
    "    print('after backward, layer1.weight.grad:' , layer1.weight.grad)\n",
    "    \n",
    "    #update weights\n",
    "    optimizer1.step()\n",
    "    optimizer2.step()\n",
    "    optimizer3.step()\n",
    "    optimizer4.step()\n",
    "    print('after train, layer.weight:', layer1.weight)\n",
    "    print(i, '-'*50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
